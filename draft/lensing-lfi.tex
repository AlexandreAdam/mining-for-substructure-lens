% !TeX root = ./lensing-lfi.tex

\documentclass[twocolumn]{aastex62}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{fontawesome} % For code link icons

\input{definitions}

\shorttitle{Inferring dark matter substructure with machine learning}
\shortauthors{Brehmer and Mishra-Sharma et al.}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\title{\textbf{
Mining for Dark Matter Substructure: \\
Inferring subhalo population properties from strong lenses with machine learning
}}

\correspondingauthors{Siddharth Mishra-Sharma}{Johann \newline Brehmer}
\email{sm8383@nyu.edu}{johann.brehmer@nyu.edu}
% \correspondingauthor{Johann Brehmer}
% \email{johann.brehmer@nyu.edu}
% \correspondingauthor{Siddharth Mishra-Sharma}
% \email{sm8383@nyu.edu}

\author[0000-0003-3344-4209]{Johann Brehmer}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}

\author[0000-0001-9088-7845]{Siddharth Mishra-Sharma}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New York, NY 10003, USA}

\author{Joeri Hermans}
\affil{Montefiore Institute, University of Li\`ege, Belgium}

\author[0000-0002-2082-3106]{Gilles Louppe}
\affil{Montefiore Institute, University of Li\`ege, Belgium}

\author[0000-0002-5769-7094]{Kyle Cranmer}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}

\begin{abstract}\noindent
The subtle and unique imprint of dark matter substructure on extended arcs in galaxy-galaxy strong lenses contains a wealth of information about the properties and distribution of dark matter on small scales and, consequently, about the underlying particle physics. However, teasing out this effect poses a significant challenge due to the high dimensionality of the underlying latent space associated with a large number of dark matter subhalos. We apply recently-developed simulation-based techniques to the problem of substructure inference in galaxy-galaxy strong lenses. By leveraging additional information extracted from the simulator, these methods can be used to train neural networks to estimate likelihood ratios associated with population-level parameters characterizing substructure. We show through proof-of-principle application to simulated data how these methods can provide an efficient and principled way to infer substructure properties by concurrently analyzing an ensemble of strong lensing images such as those deliverable by upcoming surveys. \href{https://github.com/smsharma/StrongLensing-Inference}{\faGithub}
\end{abstract}

\keywords{%
strong gravitational lensing (1643)
---
gravitational lensing (670)
---
nonparametric inference (1903)
---
astrostatistics techniques (1886)
---
cosmology (343)
---
dark matter (353)
}

\tableofcontents{}

\section{Introduction}
\label{sec:intro}

Dark matter (DM) accounts for nearly a quarter of the energy budget of the Universe, and pinning down its fundamental nature and interactions is one of the most pressing problems in cosmology and particle physics today. Despite an organized effort to do so through terrestrial~\citep{2018PhRvL.121k1302A,2017PhRvL.119r1302C,2017PhRvL.118b1303A}, astrophysical~\citep{2017ApJ...834..110A,2018PhRvD..98l3004C,2018PhRvL.120j1101L}, and collider searches~\citep{2019arXiv190301400A,2017PhLB..769..520S}, no conclusive evidence of interactions between the Standard Model (SM) and dark matter exists to-date.

An alternative and complementary approach involves studying dark matter directly through its irreducible gravitational interactions. The concordance Cold Dark Matter (CDM) framework of non-relativistic, collisionless dark matter particles provides an excellent description of the observed distribution of matter on large scales. However, many well-motivated models predict deviations from CDM on smaller scales. Fundamental dark matter microphysical properties, such as its particle mass and self-interaction cross-section, can imprint themselves onto its macroscopic distribution in ways that can be probed by current and future experiments~\citep{2019arXiv190201055D}. As motivating examples, theories where dark matter has a significant free-streaming length would lead to a dearth of subhalos at lower masses ($\lesssim 10^9\,\mathrm{M}_\odot$)~\citep{1983ApJ...274..443B,2001ApJ...556...93B,astro-ph/0004381,0807.0622,1008.0992}, and self-interactions~\citep{1508.03339,1311.6524,1211.6426,1208.3026,1201.5892,1805.03203,1904.10539} or dissipative dynamics~\citep{1706.04195,1702.05482,1707.03829,1303.1521,1512.05349} in the dark sector would modify the structure of the inner core of subhalos as compared to CDM predictions.

There exist several avenues for probing the structure of dark matter on small scales. While the detection of ultrafaint dwarf galaxies through the study of stellar overdensities and kinematics~\citep{1503.02584,0706.2687,1503.02079} can be used to make statements about the underlying dark matter properties, theoretical uncertainties in the connection between stellar and halo masses~\citep{1804.03097} and the effect of baryons on the satellite galaxy population~\citep{1812.00044,1811.11791,1701.03792,1608.01849} pose a challenge. Furthermore, suppressed star-formation in smaller halos means that there exists a threshold ($\lesssim 10^8\,\mathrm{M}_\odot$) below which subhalos are expected to be mostly dark and devoid of stars~\citep{1992MNRAS.256P..43E,1611.02281,1607.03127}. This makes studying the imprints of gravitational interaction the \emph{only} viable avenue for probing substructure at smaller scales. In this spirit, the study of perturbations to the stellar phase-space distribution in cold stellar streams~\citep{1804.06854,astro-ph/9807243,1109.6022,1303.4342,1811.03631}, and in stellar fields in the disk and halo~\citep{1711.03554} have been proposed as methods to look for low-mass subhalos through their gravitational interactions in the Milky Way.

Complementary to the study of locally-induced gravitational effects, gravitational lensing has emerged as an important tool for studying the distribution of matter over a large range of scales. Locally, the use of time-domain astrometry has been proposed as a promising method to measure the distribution of local substructure through correlated, lens-induced motions on background celestial objects~\citep{2018JCAP...07..041V}. In the extragalactic regime, galaxy-scale strong lensing systems are a laboratory for studying substructure. The presence of flux-ratio anomalies in multiply-imaged quasar lenses has been used to infer the typical abundance of substructure within galaxy-scale lenses~\citep{2002ApJ...572...25D,2019arXiv190504182H,2002ApJ...572...25D}. Lensed images of extended sources have been used to find evidence for a handful of subhalos with masses $\gtrsim 10^8\,\mathrm{M}_\odot$~\citep{1601.01388,0910.0760,1201.3643}.

Another approach relies on probing the collective effect of sub-threshold (i.\,e., not individually resolvable) subhalos on extended arcs in strongly lensed systems. A particular challenge here is the high dimensionality of the latent parameter space associated with the large number of subhalos and their (potentially covariant) individual as well as population properties, a consequence of which is the intractability of the likelihood of high-level substructure parameters conditional on the data. Methods based on summary statistics~\citep{1702.00009} and studying the amplitude of spatial fluctuations on different scales through power spectra~\citep{1403.2720,1809.00004,1707.04590,1806.07897,1808.03501,1710.03075,1506.01724} have been proposed as ways to reduce the dimensionality of the problem and enable substructure inference in a tractable way. Trans-dimensional techniques may also be able to efficiently map out the parameter space associated with multiple sub-threshold subhalos in these systems~\citep{1508.00662,1706.06111}. This class of methods is well-suited to studying dark matter substructure since they can be sensitive to the \emph{population} properties of low-mass subhalos in strongly lensed galaxies which are directly correlated with the underlying dark matter particle physics. Furthermore, near-future observatories like LSST~\citep{0912.0201,2019arXiv190201055D,1902.05141} and \Euclid~\citep{1001.0061} are expected to find tens of thousands of galaxy-galaxy strong lenses~~\citep{2015ApJ...811...20C,1001.2037,1003.5567}, making substructure inference in these systems (and high-resolution followups on a subset) one of the key ways to investigate dark matter substructure and stress-test the Cold Dark Matter paradigm in the near future. This calls for methods that can efficiently analyze large samples of lensed images to infer the underlying substructure properties with minimal loss of information stemming from dimensional reduction.

In recent years, a large number of methods have been developed that train neural networks to estimate the likelihood function, likelihood ratio function, or posterior~\cite{2012arXiv1212.1479F, 2014arXiv1410.8516D, 2015arXiv150203509G, 2015arXiv150505770J, Cranmer:2015bka,  2016arXiv160206701P, 2016arXiv160502226U, 2016arXiv160508803D, 2016arXiv160605328V, 2016arXiv160903499V, 2016arXiv160106759V, 2016arXiv161110242D, NIPS2016_6084, 2017arXiv170208896T, 2017arXiv170507057P, 2017arXiv170707113L, 2017arXiv171101861L, gutmann2017likelihood, 2018arXiv180400779H, 2018arXiv180507226P, 2018arXiv180509294L, DBLP:journals/corr/abs-1806-07366, 2018arXiv180703039K, 2018arXiv181001367G, 2018arXiv181009899D, Hermans:2019ioj, Alsing:2019xrx}.  In contrast to traditional simulation-based (or ``likelihood-free'') approaches, these methods do not rely on summary statistics and instead learn to extract information directly from the full input data, in our case observed lens images. These approaches let us to amortize the computational cost of the inference: after an upfront simulation and training phase, inference for any observed lens image is efficient, enabling a stacked analysis of a large number of observations.

We follow this approach and apply a particularly efficient technique for simulation-based inference~\citep{1805.00013, 1805.00020, 1805.12244} to the problem of extracting high-level substructure properties from an ensemble of galaxy-galaxy strong lensing images. This method extracts additional information from the simulator, which is then used to train a neural network as a surrogate for the likelihood ratio function. Compared to other neural-network-based methods, the additional information increases the sample efficiency and thus reduces the computational cost. A calibration procedure ensures correct inference results even in the case of imperfectly trained networks.

We demonstrate this method on a catalog of simulated lenses. After discussing the information in individual lens images, we switch to a stacked analysis of multiple observed images and calculate the expected limits on population-level substructure parameters in both a frequentist and a Bayesian setup.

This paper is organized as follows. In Sec.~\ref{sec:lensing-formalism} we briefly review the formalism of gravitational strong lensing and describe our simulation setup, including the assumptions we make about the population of lensed sources and host galaxies, the substructure population and observational parameters. In Sec.~\ref{sec:lfi-formalism} we describe the simulation-based analysis technique used and its particular application to the problem of mining substructure properties from an ensemble of extended lensed arcs. We show a proof-of-principle application of this method to simulated data in Sec.~\ref{sec:results} and comment on how these methods can be extended to more ``realistic'' scenarios in Sec.~\ref{sec:extensions}. We conclude in Sec.~\ref{sec:conclusions}.


\section{Strong lensing formalism and simulation setup}
\label{sec:lensing-formalism}

In strong lensing systems, the background light emission source can in general be a point-like quasar or supernova, or a faint, extended ``blue'' galaxy. The former results in multiple localized images on the lens plane rather than extended arc-like images, providing the ability to probe substructure over a limited region on the lens plane. For this reason, we focus our method towards galaxy-galaxy lenses --- systems producing images with extended arcs --- since we aim to disentangle the collective effect of a population of subhalo perturbers over multiple images. Young, blue galaxies are ubiquitous in the redshift regime $z\gtrsim1$ and dominate the faint end of the galaxy luminosity function, resulting in a much larger deliverable sample of galaxy-galaxy strong lenses compared to quadruply- and doubly-imaged quasars/supernovae.

We now briefly review the basic strong lensing formalism before describing in turn the models for the background source, lensing galaxy and population parameters of the lens systems used in this study.

\subsection{Strong lensing formalism}

We briefly review here the mathematical formalism behind strong lensing. For more details see, e.\,g.,~\cite{2001astro.ph..2341K,1992grle.book.....S}. For a mass distribution with dimensionless projected surface mass density $\kappa(\mathbf{r})=\Sigma(\mathbf{r}) / \Sigma_{\mathrm{cr}}$, where $\Sigma_{\mathrm{cr}}\equiv \frac{1}{4 \pi G_N} \frac{D_{\mathrm{s}}}{D_{\mathrm{ls}} D_{\mathrm{l}}}$ is the critical lensing surface density, the two-dimensional lensing potential is given by
\begin{equation}
\psi(\mathbf{r})=\frac{1}{\pi} \int \diff \mathbf{r^\prime}\,\ln |\mathbf{r}-\mathbf{r^\prime}| \kappa(\mathbf{y}) .
\end{equation}
The lensed position of the source can be determined through the lens equation,
\begin{equation}
\mathbf{u}=\mathbf{r}-\nabla \psi(\mathbf{r})
\end{equation}
where $\mathbf{u}$ is the position of the source and $\nabla \psi$ is typically referred to as the deflection, which we will denote as $\boldsymbol\phi$ for brevity. For an extended source brightness profile $f_\mathrm{src}$, the final lensed image can be obtained as the source profile evaluated on the image plane,
\begin{equation}
f^\prime_\mathrm{src}(\mathbf r) = f_\mathrm{src}\left(\mathbf{r}-\nabla \psi(\mathbf{r})\right).
\end{equation}
For a spherically symmetric halo, the radial deflection field is given by
\begin{equation}
\phi_{r}(r)=\frac{2}{r} \int_{0}^{r}\diff r^\prime\,r^\prime\,\kappa(r^\prime) =\frac{1}{\pi \Sigma_{\mathrm{cr}}} \frac{M_{\mathrm{cyl}}(r)}{r} \,,
\end{equation}
where $M_{\mathrm{cyl}}(r)$ is the mass enclosed within a cylinder or radius $r$. Extension to the slightly more general case of elliptical symmetry is straightforward (see, e.\,g.,~\cite{2001astro.ph..2341K}).

\subsection{Lensing host galaxy}

Cosmological $N$-body simulations suggest that the dark matter distribution in structures at galactic scales can be well-described by a universal, spherically symmetric Navarro-Frenk-White (NFW) profile. However, strong lensing probes a region of the host galaxy much smaller than the typical virial radii of galaxy-scale dark matter halo, and the mass budget here is dominated by the baryonic bulge component of the galaxy. Taking this into account, the total mass budget of the lensing host galaxy, being early-type, can be well-described by a singular isothermal ellipsoid (SIE) profile. Since neither the dark matter nor the baryonic components are individually isothermal, this is known as the bulge-halo conspiracy. The host profile is thus described as
\begin{equation}
\rho(\theta_x, \theta_y)=\frac{\sigma_{v}^{2}}{2 \pi G\left(\theta_x^{2} / q+q \theta_y^{2}\right)}
\label{eq:hostprofile}
\end{equation}
where $\sigma_{v}$ is the central 1-D velocity dispersion of the lens galaxy and $q$ is the ellipsoid axis ratio, with $q=1$ corresponding to a spherical profile. We explicitly denote our angular coordinates as $\left\{\theta_x, \theta_y\right\}$. The Einstein radius for this profile, its characteristic lensing scale, is given by
\begin{equation}
\theta_{{E}}=4 \pi\left(\frac{\sigma_{v}}{c}\right)^{2} \frac{D_{l s}\left(z_{l}, z_{s}\right)}{D_{s}\left(z_{s}\right)} \,,
\label{eq:siethetae}
\end{equation}
where $D_{ls}$ and $D_s$ are respectively the angular diameter distances from the source to the lens planes and from the source plane to the observer.

The deflection field for the SIE profile is given by~\citep{2001astro.ph..2341K}
\begin{align}
\phi_{x} &=\frac{\theta_E q}{\sqrt{1-q^{2}}} \tan ^{-1}\left[\frac{\sqrt{1-q^{2}} \theta_x}{\psi}\right] \\
\phi_{y} &=\frac{\theta_E q}{\sqrt{1-q^{2}}} \tanh ^{-1}\left[\frac{\sqrt{1-q^{2}} \theta_y}{\psi+q^{2} }\right]
\end{align}
with $\psi \equiv \sqrt{\theta_x^2 q^2 + \theta_y^2}$.

Although the total galaxy mass (baryons + dark matter) describe the macro lensing field, for the purposes of describing substructure we require being able to map the measure properties of an SIE lens onto the properties of the host dark matter halo. To do this, we relate the central stellar velocity dispersion $\sigma_v$ to the mass $M_{200}$ of the host dark matter halo. \citet{2018ApJ...859...96Z} derived a tight correlation between $\sigma_v$ and $M_{200}$, modeled as
\begin{equation}
\log\left(\frac{M_{200}}{10^{12}\,\Msun}\right) = \alpha + \beta\left(\frac{\sigma_v}{100\,\kmps}\right)
\label{eq:sigma_v_M_200_relation}
\end{equation}
with $\alpha = 0.09$ and $\beta = 3.48$. % with a mean log-normal scatter of 0.13\,dex.
We model the host dark matter halo with an NFW profile~\citep{1996ApJ...462..563N,1997ApJ...490..493N}

\begin{equation}
\rho(r)=\frac{\rho_{s}}{\left(r / r_{s}\right)\left(1+r / r_{s}\right)^{2}}
\label{eq:rhoNFW}
\end{equation}
where $\rho_s$ and $r_s$ are the scale density and scale radius, respectively. The halo virial mass $M_{200}$ describes the total mass contained with the virial radius $r_{200}$, defined as the radius within which the mean density is 200 times the critical density of the universe and related to the scale radius through the concentration parameter $c_{200} \equiv r_{200}/r_s$. Thus, an NFW halo is completely described by the parameters $\{M_{200}, c_{200}\}$. We use the concentration-mass relation from~\citet{2014MNRAS.442.2271S} assuming a log-normal distribution for $c_{200}$ around the median inferred value given by the relation with scatter 0.15\,dex.

The spherically-symmetric deflection for an NFW perturber is given by~\citep{2001astro.ph..2341K}
\begin{equation}
\phi_{r}=4 \kappa_{s} r_{s} \frac{\ln (x / 2)+\mathcal{F}(x)}{x} \,,
\label{eq:nfw_deflection}
\end{equation}
where $x \equiv r/r_s, \kappa_s\equiv \rho_s\,r_s/\Sigma_\mathrm{cr}$ with the critical surface density $\Sigma_\mathrm{cr}$, and
\begin{equation}
\mathcal{F}(x)=\left\{\begin{array}{ll}{\frac{1}{\sqrt{x^{2}-1}} \tan ^{-1} \sqrt{x^{2}-1}} & {(x>1)} \\ {\frac{1}{\sqrt{1-x^{2}}} \tanh ^{-1} \sqrt{1-x^{2}}} & {(x<1)} \\ {1} & {(x=1).}\end{array}\right.
\label{eq:Fx}
\end{equation}

We described the population parameters we use to model the host velocity dispersion (and thus its Einstein radius and dark matter halo mass) in Secs.~\ref{sec:observations} and~\ref{sec:populations} below.

\subsection{Lensing substructure}

The ultimate goal of our method is to characterize the substructure population in strong lenses. Here we describe our procedure to model the substructure contribution to the lensing signal. Understanding the expected abundance of substructure in galaxies over a large range of epochs is a complex undertaking and an active area of research. Properties of individual subhalos (such as their density profiles) as well as those that describe their population (such as the mass and spatial distribution) are strongly affected by their host environment, and accurately modeling all aspects of subhalo evolution and environment is beyond the scope of this paper. Instead, we use a simplified description to model the substructure contribution in order to highlight the broad methodological points associated with the application of our method.

 \lcdm, often called the standard model of cosmology, predicts a scale-invariant power spectrum of primordial fluctuations and the existence of substructure over a broad range of masses with equal contribution per logarithmic mass interval. We parameterize the distribution of subhalo masses $\mtwo$ in a given host halo of mass $\Mtwo$ --- the subhalo mass function --- as a power law distribution with a linear dependence on the host halo mass,
\begin{equation}
\frac{\diff n}{\diff \log \frac {\mtwo}{m_{200,0}}} =
\begin{cases}
  \alpha \frac{\Mtwo}{M_{200,0}} \!\left(\!\frac{\mtwo}{m_{200,0}}\!\right)^{\!\beta} & \scriptstyle (m_\mathrm{200}^\mathrm{min} \leq \mtwo \leq m_\mathrm{200}^\mathrm{max}) \\
  0 & \scriptstyle (\text{else})\,,
\end{cases}
\label{eq:shmf}
\end{equation}
where $\alpha$ encodes the overall substructure abundance, with larger $\alpha$ corresponding to more substructure, and the slope $\beta < 0$ encodes the relative contribution of subhalos at different masses, with more negative $\beta$ corresponding to a steeper slope with more low-mass subhalos. $m_{200, 0}$ and $M_{200, 0}$ are arbitrary normalization factors. %The normalization factors $m_{200, 0}$ and $M_{200, 0}$ are arbitrarily set to $10^9\,\Msun$ and the Milky Way mass $\MMW \simeq 1.1\times10^{12}\,\Msun$, respectively.

Theory and simulations within the framework of \lcdm~predict a slope $\beta\approx-0.9$~\citep{0809.0898,0802.2265}, giving a nearly scale-invariant spectrum of subhalos, which we assume in our fiducial setup.
% We follow the specifications in~\citet{2016JCAP...09..047H} in order to set the overall fiducial abundance of subhalos, normalizing $\alpha$ to give 150 subhalos in expectation between $10^{8}\,\Msun$ and $10^{10}\,\Msun$ for a Milky Way-sized host halo.
We parameterize the overall subhalo abundance $\alpha$ through the mass fraction contained in subhalos, $f_\mathrm{sub}$, defined as the fraction of the total dark matter halo mass contained in bound substructure in a given mass range. We have
\begin{equation}
f_\mathrm{sub} = \frac{\int_{m_\mathrm{200, min}}^{m_\mathrm{200, max}}\diff \mtwo\,\mtwo\,\frac{\diff n}{\diff \mtwo}}{M_\mathrm{200}}
\end{equation}
For a given $\left\{f_\mathrm{sub},\beta\right\}$ and host halo mass $M_\mathrm{200}$, this can be used to determine $\alpha$ in Eq.~\eqref{eq:shmf}. The linear scaling of the subhalo mass function with the host halo mass $\Mtwo$ in Eq.~\eqref{eq:shmf} is additionally described in~\citet{2016MNRAS.457.1208H,2017MNRAS.469.1997D}. In our fiducial setups, we take the minimum mass $m_\mathrm{200, min} = 10^7\,\Msun$ and $m_\mathrm{200, max} = 0.01\,\,M_\mathrm{200}$~\citep{2017MNRAS.469.1997D,2018PhRvD..97l3002H}, and corresponding fiducial substructure fraction in this range of 5\%, roughly consistent with~\citet{2018PhRvD..97l3002H,2019arXiv190504182H,2002ApJ...572...25D} within our considered mass range.

With all parameters of the subhalo mass function specified, the total number of subhalos $\mean{n}_{\mathrm{tot}}$ expected within the virial radius $\Rtwo$ of the host halo can be inferred as $\int_{m_\mathrm{200, min}}^{m_\mathrm{200, max}}\diff \mtwo\,\frac{\diff n}{\diff \mtwo}$. Strong lensing probes a region much smaller than this scale --- the typical Einstein radii for the host deflector are much smaller than the virial radius of the host dark matter halos. In order to obtain the expected number of subhalos within the lensing observations region of interest (ROI), we scale the total number of subhalos obtained from the above procedure by the ratio of projected mass within our region of interest $\theta_\textrm{ROI}$ and the host halo mass $\Mtwo$ as follows. We assume the subhalos to be distributed in number density following the host NFW dark matter profile. In this case, the NFW enclosed mass function is $M_\mathrm{enc}(x) = \Mtwo\left[\ln(x/2) + \mathcal{F}(x)\right]$~\citep{2001astro.ph..2341K}, where $x$ is the angular radius in units of the virial radius, $x\equiv \theta/\theta_s$ and $\mathcal{F}(x)$ is given by Eq.~\eqref{eq:Fx} above. The expected number of subhalos within our ROI is thus obtained as $\mean n_\mathrm{ROI} = \mean n_\mathrm{tot}\left[\ln(x_\mathrm{ROI}/2) + \mathcal{F}(x_\mathrm{ROI})\right]$. We conservatively take the lensing ROI to enclose a region of angular size twice the Einstein radius of the host halo, $\theta_\mathrm{ROI} = 2\cdot\theta_E$.

Since strong lensing probes the line-of-sight distribution of subhalos within the host, their projected spatial distribution is approximately uniform within the lensing ROI~\citep{2017MNRAS.469.1997D}. We thus distribute subhalos uniformly within our ROI. The density profile of subhalos is assumed to be NFW and given by Eq.~\eqref{eq:rhoNFW}, with associated lensing properties as described and the concentration inferred from using~\citet{2014MNRAS.442.2271S}.

We finally emphasize that we do not intent to capture all of the intricacies of the subhalo distribution, such as the effects of baryonic physics, tidal distruption of subhalos in proximity to the center of the host and redshift evolution of host as well as substructure properties. Although our description can be extended to take these into account, their precise characterization and effect is still subject to large uncertainties, and our simple model above captures the essential physics for demonstration purposes.

\subsection{Background source}
\label{sec:source}

We model the emission from background source galaxies using a S\'{e}rsic profile, with the surface brightness given by~\citep{1963BAAA....6...41S}
\begin{equation}
f_\mathrm{src}(\theta_r)=f_{e} \exp \left\{-b_{n}\left[\left(\frac{\theta_r}{\theta_{r,e}}\right)^{1 / n}-1\right]\right\},
\end{equation}
where $\theta_{r,e}$ is the effective circular half-light radius, $n$ is the S\'{e}rsic index, and $b_n$ is a factor depending on $n$ that ensures that $\theta_{r,e}$ contains half the total intensity from the source galaxy, given by~\citep{1999A&A...352..447C}
\begin{align}
b_n \approx 2 n &- \frac{1}{3} + \frac{4}{405 n} + \frac{46}{25515 n^2} \nonumber \\ &+ \frac{131}{1148175 n^3} - \frac{2194697}{30690717750 n^4}. \nonumber
\end{align}

We assume $n=1$ for the source galaxies, corresponding to a flattened exponential profile and consistent with expectation for blue-type galaxies at the relevant redshifts. $f_{e}$ encodes the flux at half-light radius, which can be mapped onto the total flux (or magnitude) associated with a given galaxy.

The total unlensed magnitude $M$ (in a given band) of a galaxy can be mapped on to $f_{e}$ as follows. For a detector with zero-point magnitude $M_0$, which specifies the magnitude of a source giving 1 count\,s$^{-1}$ in expectation, by definition the total counts are given by $S_\mathrm{tot}=10^{0.4(M-M_0)}$. Requiring the half-light radius to contain half the expected counts, for $n=1$ we have the relation $f_{e} \approx 0.526\,t_\mathrm{exp}S_\mathrm{tot} /(2\pi \theta_{r,e}^2)$ in counts\,arcsec$^{-2}$, where $t_\mathrm{exp}$ is the exposure length.

The treatment of the other S\'{e}rsic parameters, in particular the total emission and half-light radius, in the context of population studies is described in Secs.~\ref{sec:observations} and~\ref{sec:populations} below.

\subsection{Observational considerations}
\label{sec:observations}

A noted above, our method is best-suited to analyzing a statistical sample of strong lenses to search for substructure, such as those that are expected to be obtained in the near future with optical telescopes like \Euclid~and LSST. Given the challenges associated with the precise characterization of such a sample at the present time, we describe here the observational characteristics we assume in order to build up training and testing samples to validate our inference techniques.

We largely follow the description of~\citet{2015ApJ...811...20C}, and use the associated \package{LensPop} package, to characterize our mock observations. In particular, we use the detector configuration for \Euclid, assuming a zero-point magnitude $m_\mathrm{AB} = 25.5$ in the single optical VIS passband, pixel size 0.1\,arcsec, a Gaussian point spread function (PSF) with FWHM 0.18\,arcsec, individual exposures with exposure time 1610\,s, and an isotropic sky background with magnitude 22.8\,arcsec$^{-2}$ in the detector passband.

These properties, in particular the exposure, sky background, and PSF shape, are expected to vary somewhat across the lens sample. Additionally, a given region may be imaged by multiple exposures over a range of color bands. Although these variations can easily be incorporated into our analysis, modeling this is beyond the scope of this study. We briefly comment on how this information can be taken into account later.

\subsection{Population statistics of the lens and source samples}
\label{sec:populations}

The fact that the strong lens population is expected to be dominated by higher-redshift ($z\gtrsim1$) blue source galaxies lensed by intermediate-redshift ($z\sim 0.5$--$1$) elliptical galaxies presents significant challenges for quantifying the lens population obtainable with future observations. Specifically, planned ground-based surveys like LSST and space telescopes like \Euclid~present complementary challenges for delivering images of strong lensing systems suitable for substructure studies. LSST is expected to image in six bands, allowing efficient source selection and distinguishing source and lens emission, but at the cost of lower resolution by virtue of being a ground-based instrument. \Euclid~imaging is expected be much higher in resolution but with a single optical passband (VIS). Near-IR imaging from WFIRST may deliver a high-resolution, multi-wavelength dataset that is more suitable for substructure studies, although the lens and source populations may differ from those probed by optical telescopes.

% In light of these uncertainties, we limit the scope of the present study to developing a class of methods that can be adapted to the specifications of a galaxy-galaxy strong lensing population obtained with the next generation of optical, near-IR and radio telescopes. In particular, we confine ourselves to a setting where the main methodological points can be made without detailed modeling of the detector capabilities and the deliverable lensing dataset, which is outside of the scope of the current paper.

In light of these uncertainties, we confine ourselves to a setting where the main methodological points can be made without detailed modeling of the detector capabilities and the deliverable lensing dataset, which is outside of the scope of the current paper. For concreteness, we simulate a sample of lenses with a simplified subset of host galaxy properties consistent with those deliverable by \Euclid~as modeled by~\citet{2015ApJ...811...20C}. In particular, we assume spherical lenses, with ellipticity parameter $q=1$ in Eq.~\eqref{eq:hostprofile}. We draw the central 1-D velocity dispersions $\sigma_v$ of host galaxies from a normal distribution with mean 225\,km\,s$^{-1}$ and standard deviation 50\,km\,s$^{-1}$. Following Eq.~\eqref{eq:sigma_v_M_200_relation}, the results of~\citet{2018ApJ...859...96Z} are used to map the drawn $\sigma_v$ to a dark matter halo mass $\Mtwo$, and the host Einstein radius is analytically inferred with Eq.~\eqref{eq:siethetae}.

We draw the lens redshifts $z_l$ from a log-normal distribution with mean 0.56 and scatter 0.25 dex, discarding lenses with $z_l > 1$ as these tend to have a small angular size over which substructure perturbations are relevant. The source redshift is fixed at $z_\mathrm{source} = 1.5$, its offsets $\theta_x$ and $\theta_y$ are drawn from a normal distribution with zero mean and standard deviation 0.2. These are consistent with the lens sample generated from the \package{LensPop} code packaged with~\citet{2015ApJ...811...20C}.

\begin{figure*}
\centering
\includegraphics[width=1.\textwidth]{figures/simulations}
\caption{Simulated lenses. The cross markers show the offset of the host galaxy, its color its mass. The simulated subhalos are shown as dots, the color again indicates their masses. The greyscale images show the corresponding observed images. We show seven images randomly generated for $\fsub = 0.05$ and $\beta = -0.9$.}
\label{fig:simulations}
\end{figure*}


\section{Statistical formalism and simulation-based inference}
\label{sec:lfi-formalism}

Our goal is to infer the subhalo mass function parameters from a catalog of images of observed lenses. In this section we will describe the challenges of this inference problem and our approach of simulation-based inference. For simplicity, we will use a more abstract notation, distinguishing between three sets of quantities in the lensing system:
%
\begin{description}
  \item[Parameters of interest $\stattheta$] The vector $\stattheta = (\fsub, \beta)^T$ parameterizes the subhalo mass function given, our goal is to infer their values.
  %
  \item[Latent variables $z$] A vector of all other unobservable random variables in the simulator. These include the mass $M_{200}$, offset $(\theta_x, \theta_y)$, and redshift $z_\mathrm{host}$ of the host galaxy, the number of subhalos in the region of interest $n_\mathrm{ROI}$, the position $\mathbf{r}$ and mass $m_{200}$ of each subhalo, and the random variables related to the point spread function and Poisson fluctuations.
  %
  \item[Observables $x$] The observed lens images.
\end{description}
%
Unfortunately, the same symbols are used with different meanings in astrophysics and statistics: note the difference between the parameters $\vartheta$ and the angular positions $\theta_x$, $\theta_y$ and the Einstein radius $\theta_E$; between the latent variables $z$ and the redshifts $z_\mathrm{source}$, $z_\mathrm{host}$; and between the observed image $x$ and the argument of the NFW profile $M_\mathrm{enc}(x)$ used in the last section.

As described above, we have implemented a simulator for the lensing process in the ``forward'' direction: for given parameters $\stattheta$, the simulator samples latent variables $z$ and finally observed images $x \sim p(x|\stattheta)$. Here $p(x|\stattheta)$ is the probability density or likelihood function of observing a lens image $x$ given parameters $\stattheta$. It can be schematically written as
%
\begin{equation}
 p(x|\stattheta) = \int\!\diff z \; p(x, z|\stattheta) \,,
 \label{eq:likelihood_latent}
\end{equation}
%
where we integrate over the latent variables $z$ and $p(x, z|\stattheta)$ is the joint likelihood of observables and latent variables:
%
\begin{multline}
  p(x,z | \stattheta)
  = p_\mathrm{host}(M_{200}, \theta_x, \theta_y, z_\mathrm{host}) \\
  \times  \pois(n_\mathrm{ROI} | \mean{n}_\mathrm{ROI}(\stattheta)) \prod_i^{n_\mathrm{ROI}} \Bigl[ p_m \! \left( m_{200, i} \middle| \stattheta \right) \; \uniform \left( \mathbf{r}_i \right) \Bigr] \\
  \times p_\mathrm{obs} ( x | f(M_{200}, \theta_x, \theta_y, z_\mathrm{host}; \{(m_{200, i},\mathbf{r}_i)\})) \,.
  \label{eq:joint_likelihood}
\end{multline}
%
Here $p_\mathrm{host}(M_{200}, \theta_x, \theta_y, z_\mathrm{host})$ is the distribution of the host halo parameters; $\mean{n}_\mathrm{ROI}(\stattheta)$ is the mean number of subhalos in the region of interest as a function of the parameters $\stattheta = (\fsub, \beta)^T$, while $n_\mathrm{ROI}$ is the actually realized number in the simulation; $m_{200, i}$ and $\mathbf{r}_i$ are the subhalo masses and positions; $p_m(m|\stattheta) = 1/n \, \diff n / \diff m_{200}$ is the normalized subhalo mass function given in Eq.~\eqref{eq:shmf}; and in the last line $p_\mathrm{obs}$ is the probability of observing an image $x$ based on the true lensed image $f(z_\mathrm{host},\{(m_{200, i},r_i)\})$ taking into account Poisson fluctuations and point spread function.

Most frequentist and Bayesian inference methods rely on evaluating the likelihood function $p(x|\stattheta)$. Unfortunately, even in our somewhat simplified simulator, each run of the simulation easily involves hundreds to thousands of latent variables, and the integral over this enormeous space clearly cannot be computed explicitly. The likelihood function $p(x | \stattheta)$ is thus intractable, providing a major challenge for both frequentist and Bayesian inference. Similarly, inference with Markov Chain Monte Carlo (MCMC) methods based directly on the joint likelihood function $p(x,z | \stattheta)$ requires unfeasibly many simulations before converging because the latent space is so large. Systems defined through a forward simulator that does not admit a tractable likelihood are known as ``implicit models'', inference techniques for this case as ``simulation-based inference'' or ``likelihood-free inference''.

One way to tackle this issue is to reduce the high-dimensional data $x$ to lower-dimensional summary statistics $v(x)$, for instance based on power spectra~\citep{1403.2720,1809.00004,1707.04590,1806.07897,1808.03501,1710.03075,1506.01724}. The likelihood $p(v|\stattheta)$ in the space of summary statistics can either be explicitly estimated through density estimation techniques such as histograms, kernel density estimation, or Gaussian processes, or replaced by a rejection probability in an Approximate Bayesian Computation (ABC) technique~\citep{rubin1984}. While the compression to summary statistics makes the analysis tractable, it typically loses information and hence reduces the statistical power of the analysis.

Instead, we follow an approach in which we approximate the likelihood function with a neural network, which has to be trained only once and can be evaluated efficiently for any parameter point and observed image~\citep{2012arXiv1212.1479F, 2014arXiv1410.8516D, 2015arXiv150203509G, 2015arXiv150505770J, Cranmer:2015bka,  2016arXiv160206701P, 2016arXiv160502226U, 2016arXiv160508803D, 2016arXiv160605328V, 2016arXiv160903499V, 2016arXiv160106759V, 2016arXiv161110242D, NIPS2016_6084, 2017arXiv170208896T, 2017arXiv170507057P, 2017arXiv170707113L, 2017arXiv171101861L, gutmann2017likelihood, 2018arXiv180400779H, 2018arXiv180507226P, 2018arXiv180509294L, DBLP:journals/corr/abs-1806-07366, 2018arXiv180703039K, 2018arXiv181001367G, 2018arXiv181009899D, Hermans:2019ioj, Alsing:2019xrx}. We will show how this turns the intractable integral in Eq.~\eqref{eq:likelihood_latent} into a tractable minimization problem and amortizes this marginalization. This approach scales well to the expected large number of lenses expected in upcoming surveys. Since the full image is used as input, no information is lost due to dimensionality reduction.

We use a simulation-based inference technique introduced in \citet{1805.00013,1805.00020,1805.12244} that extracts additional information from the simulation and uses it to improve the sample efficiency of the training of the neural network. Our inference strategy consists of four steps:
%
\begin{enumerate}
  \item During each run of the simulator, additional information that characterizes the subhalo population and lensing process is stored together with the simulated observed image.
  \item This information is used to train a neural network to approximate the likelihood ratio function.
  \item The neural network output is calibrated, ensuring that errors during training do not lead to wrong inference results.
  \item The calibrated network output is then used in either frequentist or Bayesian inference techniques.
\end{enumerate}
%
In the remainder of this section, we will explain these four steps in detail.


\subsection{Extracting additional information from the simulator}
\label{sec:lfi-gold}

In a first step, we generate training data by simulating a large number of observed lenses. For each lens, we first draw two parameter points from a proposal distribution, $\stattheta, \stattheta' \sim \pi(\stattheta)$. This proposal distribution should cover the region of interest in the parameter space, but does not have to be identical to the prior in a Bayesian inference setting, which allows us to be agnostic about the inference setup at this stage.

Next, the simulator is run for the parameter point $\stattheta$, generating an observed image $x \sim p(x|\stattheta)$. In addition, we calculate and save two quantities: the joint likelihood ratio
%
\begin{equation}
  r(x,z | \stattheta) = \frac {p(x,z | \stattheta)} {p_\mathrm{ref}(x,z)}
  %\equiv \frac {p(x,z | \stattheta)} {\int\!\diff\stattheta' \, \pi(\stattheta') \, p(x,z | \stattheta') }
\end{equation}
%
and the joint score
%
\begin{equation}
  t(x, z | \stattheta) = \nabla_{\stattheta} \log p(x,z | \stattheta) \,.
\end{equation}
%
The joint likelihood ratio quantifies how much more or less likely a particular simulation chain including the latent variables $z$ is for the parameter point $\stattheta$ compared to a reference distribution
%
\begin{equation}
  \pref = \int\!\diff\stattheta' \, \pi(\stattheta') \, p(x,z | \stattheta') \,,
\end{equation}
%
where we choose the marginal distribution of latent variables and observables corresponding to the proposal distribution $\pi(\stattheta)$. Unlike the distribution for a single reference parameter point, this marginal model has support for every potential outcome of the simulation~\citep{Hermans:2019ioj}. The joint score is the gradient of the joint log likelihood in model parameter space and quantifies if a particular simulation chain becomes more or less likely under infinitesimal changes of the parameters of interest. Both quantities depend on the latent variables of the simulation chain.

We compute the joint likelihood ratio and joint score with Eq.~\eqref{eq:joint_likelihood}. Conveniently, the first and third line of that equation do not explicitly depend on the parameters of interest $\stattheta$ and cancel in the joint likelihood ratio and joint score; the remaining terms can be evaluated with little overhead to the simulation code. We also calculate the joint likelihood ratio $r(x,z|\stattheta')$ and the joint score $t(x,z|\stattheta')$ for the second parameter point $\stattheta'$ and store the parameter points $\stattheta$ and $\stattheta'$, the simulated image $x$, as well as the joint likelihood ratios and joint scores.

Our training samples consist of $10^6$ images, with parameter points chosen from a uniform range in $0.001 < \fsub < 0.2$ and $-1.5 < \beta < -0.5$.


\subsection{Machine learning}
\label{sec:lfi-ml}

How are the joint likelihood ratio and joint score, which are conditional on the latent variables $z$, useful for inference based on the likelihood function $p(x|\stattheta)$, which only depends on the observed lens images and the parameters of interest? Consider the functional
%
\begin{multline}
  L[g(x, \stattheta)] = \int\!\diff\stattheta\! \int\!\diff\stattheta'\! \int\!\diff x\! \int\!\diff z \; \pi(\stattheta) \; \pi(\stattheta') \; p(x,z|\stattheta) \\
    \times \Biggl[
    - s \log g  - (1 - s) \log (1 - g) - s' \log g'  - (1 - s') \log (1 - g') \\
    + \alpha \Bigl\{ \left| t - \nabla_\stattheta \log \tfrac{1 - g}g \Bigr|_{\stattheta}  \right|^2
    + \left| t' - \nabla_\stattheta \log \tfrac{1 - g}g \Bigr|_{\stattheta'} \right|^2 \Bigr\}
   \Biggr]  \,,
   \label{eq:alices_loss}
\end{multline}
%
where we have abbreviated $s \equiv s(x,z|\stattheta) \equiv 1 / (1 + r(x,z|\stattheta))$,  $s' \equiv s(x,z|\stattheta') \equiv 1 / (1 + r(x,z|\stattheta'))$, $g \equiv g(x, \stattheta)$, $g' \equiv g(x, \stattheta')$, $t = t(x,z | \stattheta)$, and $t' \equiv t(x,z | \stattheta')$ for readability. Note that the test function $g(x, \stattheta)$ is a function of $x$ and $\stattheta$ only. The first two lines are an improved version of the cross-entropy loss, in which the joint likelihood ratio is used to decrease the variance compared to the canonical cross-entropy~\citep{Stoye:2018ovl}. The last line adds gradient information, weighted by a hyperparameter $\alpha$.

As shown in \citet{Stoye:2018ovl}, this ``ALICES'' loss functional is minimized by the function
%
\begin{equation}
  g^*(x, \stattheta) \equiv \argmin_g L[g(x, \stattheta)] = \frac 1 {1 + r(x|\stattheta)} \,,
  \label{eq:loss_minimum}
\end{equation}
%
one-to-one with the likelihood ratio function
%
\begin{equation}
  r(x|\stattheta)
  \equiv \frac {p(x|\stattheta)} {p_\mathrm{ref}(x)}
  = \frac {1 - g^*(x, \stattheta)}{g^*(x, \stattheta)} \,.
\end{equation}
%
We demonstrate the minimization of this functional explicitly in Appendix~\ref{app:variation}. This means that if we can construct the functional in Eq.~\eqref{eq:alices_loss} with the joint likelihood ratio and joint score extracted from the simulator and numerically minimize it, the resulting function lets us reconstruct the (otherwise intractable) likelihood ratio function $r(x|\stattheta)$! Essentially, this step lets us integrate out the dependence on latent variables $z$ from the joint likelihood ratio and score, but in a general, functional form that does not depend on a set of observed images.

This is why extraction of the joint likelihood ratio and joint score has been described with the analogy of ``mining gold'' from the simulator~\citep{1805.12244}: while calculating these quantities may require some effort and changes to the simulator code, through the minimization of a suitable functional they allow us to calculate the otherwise intractable likelihood ratio function.

In practice, we implement this minimization with machine learning. A neural network plays the role of the test function $g(x, \stattheta)$, the integrals in Eq.~\eqref{eq:alices_loss} are approximated with a sum over training data sampled according to $\pi(\stattheta) \pi(\stattheta') p(x,z|\stattheta)$, and we minimize the loss numerically through a stochastic gradient descent algorithm. The neural network trained in this way provides an estimator $\hat{r}(x|\stattheta)$ of the likelihood ratio function that is exact in the limit of infinite training samples, sufficient network capacity, and efficient minimization. Note the ``parameterized'' structure of the network, in which a single neural network is trained to estimate the likelihood ratio over all of the parameter space, with the tested parameter point $\stattheta$ being an input to the network \citep{Cranmer:2015bka, Baldi:2016fzo}. This approach is more efficient than a point-by-point analysis of a grid of parameter points: it allows the network to ``borrow'' information from neighboring parameter points, benefitting from the typically smooth structure of the parameter space.

Given the image nature of the lensing data, we choose a convolutional network architecture based on the ResNet-18 \citep{he2016deep} implementation in PyTorch~\citep{paszke2017automatic}. The parameters $\stattheta$ enter as additional inputs in the fully connected layers of the network. Compared to the original ResNet-18 architecture, we add another fully connected layer at the end to ensure that the relation between parameters of interest and image data can be modeled. All inputs are normalized to zero mean and unit variance. We train the networks by minimizing the loss in Eq.~\eqref{eq:alices_loss} with $\alpha = 2 \cdot 10^{-3}$ over 100 epochs with a batch size of 128 using the Adam optimizer~\citep{2014arXiv1412.6980K}, exponentially decaying the learning rate from $3\cdot 10^{-4}$ to $3 \cdot 10^{-6}$ with early stopping. This architecture and hyperparameter configuration performed best during a rough hyperparameter scan, though for this proof-of-concept study we have not performed an exhaustive optimization.


\subsection{Calibration}
\label{sec:lfi-calibration}

In reality, the neural network might not learn the likelihood ratio function $r(x|\stattheta)$ exactly, for instance due to limited training data or inefficient training. To make sure that our inference results are correct even in this case, we calibrate the network output with histograms~\citep{Cranmer:2015bka, 1805.00020}. For every parameter point $\stattheta$ that we want to test, we simulate a set of images $\{x\} \sim p(x|\stattheta)$ from this parameter point and calculate the network prediction $\hat{r} \equiv \hat{r}(x|\stattheta)$ for each image. We also simulate a set of images $\{x\} \sim p_{\mathrm{ref}}(x)$ from the reference model, again calculating the network prediction $\hat{r}$ for each lens. The calibrated likelihood ratio is then calculated from histograms of the network predictions as
%
\begin{equation}
  \hat{r}_\mathrm{cal}(x|\stattheta)
  = \frac {\hat{p}( \hat{r} | \stattheta )} {\hat{p}_\mathrm{ref}(\hat{r})}
\end{equation}
%
where the $\hat{p}(\dots)$ denote probability densities estimated with univariate histograms.

This additional calibration stage comes with a certain computational cost that increases linearly with the number of evaluated parameter points. However, it guarantees that as long as the simulator accurately models the process, the inference results may be perfect or conservative, but not be too optimistic, even if the neural network output is substantially different from the true likelihood ratio. As a compromise, the calibration procedure can be used for selected parameter points as a cross-check of the fidelity of the network output.

We will show results both without and with calibration. Where calibration is used, it is based on histograms with 60 bins, with bin boundaries determined automatically to match the distribution of likelihood ratios.


\subsection{Inference}
\label{sec:lfi-inference}

After a neural network has been trained (and optionally calibrated) to estimate the likelihood ratio function, it provides the basic ingredient to both frequentist and Bayesian inference. For frequentist hypothesis tests, the likelihood ratio provides the most powerful test statistic~\citep{1933RSPTA.231..289N}. In addition, its asymptotic properties allow us in many cases to directly translate a value of the likelihood ratio into a $p$-value and thus into exclusion limits at a given confidence level~\citep{Wilks:1938dza, Wald, Cowan:2010js}.

For Bayesian inference, note that we can write Bayes' theorem as
%
\begin{align}
  p(\stattheta | \{x_i\})
  &= \frac {p(\stattheta) \; \prod_i p(x_i | \stattheta)} {\int\!\diff \stattheta' \, p(\stattheta') \, \prod_i p(x_i | \stattheta')} \notag \\
  &= p(\stattheta) \Biggl[
    \int\!\diff\stattheta' \, p(\stattheta') \, \prod_i \frac {p(x_i | \stattheta')}{p(x_i | \stattheta)}
  \Biggr]^{-1} \notag \\
  &\approx p(\stattheta) \Biggl[
    \int\!\diff\stattheta' \, p(\stattheta') \, \prod_i \frac {\hat{r}(x_i | \stattheta')}{\hat{r}(x_i | \stattheta)}
  \Biggr]^{-1} \,,
  \label{eq:bayesian_post}
\end{align}
%
where $\{x_i\}$ is the set of observed lens images and $p(\stattheta)$ is the prior on the parameters of interest, which may be different from the proposal distribution $\pi(\stattheta)$ used during the generation of training data. The posterior can thus be directly calculated given an estimator $\hat{r}$, provided that the space of the parameters of interest is low-dimensional enough to calculate the integral, or with MCMC or variational inference techniques otherwise.

\bigskip
While our approach to inference is strongly based on the ideas in \citet{1805.00013, 1805.00020, 1805.12244, Stoye:2018ovl}, there are some novel features in our analysis that we would like to highlight briefly. Unlike in those earlier papers, we use a marginal model based on the proposal distribution $\pi(\stattheta)$ as reference model in the denominator of the likelihood ratio, which substantially improves the numerical stability of the algorithm. This choice also allows us to include the ``flipped'' terms with $s'$ and $g'$ in the loss function in Eq.~\eqref{eq:alices_loss}; we found that this new, improved version of the ALICES loss improves the sample efficiency of our algorithms. Both of these improvements are inspired by \citet{Hermans:2019ioj}. Finally, this is the first application of the ``gold mining'' idea to image data, the first combination with a convolutional network architecture, and the first use for Bayesian inference.


\section{Results}
\label{sec:results}

\begin{figure*}
\centering
\includegraphics[width=1.\textwidth]{figures/individual_lens_predictions}
\caption{Four simulated lens images (upper panels) and the corresponding likelihood ratio maps estimated by the network (lower panels, without calibration). The star marks the true point used to generate the images, the black line shows $95 \%$~CL contours in parameter space based on each image.}
\label{fig:individual_predictions}
\end{figure*}

After training the neural network using simulations described in Sec.~\ref{sec:lensing-formalism} and the formalism described in Sec.~\ref{sec:lfi-formalism}, we can run the inference step on a given ensemble of images to extract the likelihood ratio estimates $\hat r(x | \vartheta)$ associated with the substructure parameters of interest $\{f_\mathrm{sub}, \beta\}$. We start by illustrating in Fig.~\ref{fig:individual_predictions} inference on individual simulated lensed images realizing substructure corresponding to benchmark parameters $\beta = -0.9$ and $\fsub = 0.05$. The top row shows example simulated images, with the corresponding inferred 2-D likelihood surfaces shown in the bottom row. The true parameter point is marked with a star and the 95\% confidence limit (CL) contours are shown.

Several interesting features can already be seen here. The 95\% CL contours contain the true parameter point, with the overall likelihood surface being strongly correlated with the given image. A smaller projected surface area of the lensed arc, resulting from a smaller host halo or a larger offset between the host and source centers, generally results in a flatter likelihood surface. This is expected, since a smaller host galaxy will contain relatively less substructure, and a smaller host or larger relative offset will result in a smaller effective arc area over which the substructure can imprint itself. The third column of Fig.~\ref{fig:individual_predictions} shows an example of such a system. In contrast, the second and last columns show systems with a relatively massive host and a small offset, producing a symmetric image with a larger effective arc surface area over which the effects of substructure can be discerned. This results in ``peakier'' inferred likelihood surface, corresponding to a higher sensitivity to $\fsub$ and $\beta$.

The individual likelihood estimates can be combined in a straightforward manner into a stacked test statistic, yielding a combined analysis of an ensemble of lenses. The expected likelihood surface per-image in the asymptotic limit is shown in Fig.~\ref{fig:expected_likelihood_2d}, with the 1-D slice corresponding to $\beta = -0.9$ shown in Fig.~\ref{fig:expected_likelihood_1d}. The 95\% CL expected exclusion limits for 5, 20, and 100 lenses are shown using the dotted, dashed, and solid lines respectively. The procedure can easily be extended to an arbitrarily large collection of lenses, providing an unbiased estimate of the underlying substructure properties.

\begin{figure}
\centering
\includegraphics[height=0.4\textwidth]{figures/expected_likelihood_map}
\caption{Expected per-lens likelihood ratio map assuming $\beta = -0.9$ and $\fsub = 0.05$ in the two-dimensional parameter space. The lines show expected $95\%$~CL exclusion limits for 5 (dotted), 20 (dashed), and 100 (solid) observed lenses. While the colormap shows the network output without calibration, the lines include the calibration procedure described in Sec.~\ref{sec:lfi-calibration}.}
\label{fig:expected_likelihood_2d}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=0.4\textwidth]{figures/expected_likelihood_slice}
\caption{Expected per-lens likelihood ratio along a one-dimensional slice at $\beta = -0.9$.}
\label{fig:expected_likelihood_1d}
\end{figure}

We find that, at least within the simplifying assumptions of our simulator, an analysis of a few tens of lenses is already sensitive to the overall substructure abundance parameterized by $\fsub$. A larger observed lens sample provides a tighter constraint on substructure properties. Approximately 100 lens images are required to begin resolving $\beta$. The expected exclusion contours are centered around the true values, confirming that our inference methods yield unbiased results. Note the ``banana'' shape of the expected exclusion limits, which approximately traces the deflection due to the substructure. We demonstrate this in Fig.~\ref{fig:banana}, where we show the expected approximate deflection
%
\begin{equation}
  \tilde{\phi}_r = \sum_{\text{subhalos}} 4 \kappa_s r_s \,,
  \label{eq:approximate_deflection}
\end{equation}
%
equal to the space-independent part of Eq.~\eqref{eq:nfw_deflection}, and compare it to the expected exclusion limits.

\begin{figure}
\centering
\includegraphics[height=0.4\textwidth]{figures/warum_ist_die_banane_krumm}  % yeah, why is it?
\caption{Expected approximate deflection $\tilde{\phi}_r$ as defined in Eq.~\ref{eq:approximate_deflection} as a function of $\fsub$ and $\beta$. The solid white lines show contours of constant $\tilde{\phi}_r$, while the dotted black lines show the expected exclusion limits from Fig.~\ref{fig:expected_likelihood_2d}.}
\label{fig:banana}
\end{figure}

With the likelihood ratio in hand, a Bayesian interpretation is easily admitted using Eq.~\eqref{eq:bayesian_post}. In Fig.~\ref{fig:bayesian_post} we show the derived posterior, assuming either a uniform prior between $0.001$ and $0.2$ on $\fsub$ times a Gaussian prior with mean $-0.9$ and standard deviation $0.1$ on the slope $\beta$. This choice is intended to capture a prior expectation on the subhalo mass function slope consistent with the Cold Dark Matter scenario~\citep{0809.0898,0802.2265}. As expected from the likelihood maps, we find a posterior density peaked around the true point.


\section{Extensions}
\label{sec:extensions}

For the proof-of-concept analysis presented here our lensing simulation makes a number of simplifying assumptions in order to highlight the broad methodological points in a computationally tractable setting. An application of our method to real lensing data will invariable require extensions in our simulation and inference pipeline to account for the vast physical diversity in host and source galaxy morphologies, as well as ways to deal with more realistic detector response. Modeling substructure in a more involved way than presented here (e.\,g., to account for tidal evolution and/or suppression of small-scale structure), and accounting for substructure along the line of sight is also desired. We will now discuss these features and comment on how they might affect our pipeline and the results presented here, leaving implementation and application to real lensing data to future work.

First, we currently fix all properties of the background source as described in Sec.~\ref{sec:source}. It is straightforward to instead draw and marginalize over the parameters associated with a chosen source light distribution parameterization, with Gaussian and S\'{e}rsic~\citep{1963BAAA....6...41S} profile models being common choices. For high-fidelity images (e.\,g., those obtainable by targeted followups or interferometric imaging) more complicated features in the background galaxies such as outflows and dust-obscured regions may not be adequately captured by such a parameterization and could introduce degeneracies with the effects of substructure. Alternative parameterizations using shapelet basis sets~\citep{1803.09746, 1504.07629}, and methods based on regularized linear inversion on grids~\citep{1708.07377, astro-ph/0601493,2003ApJ...590..673W} have been introduced as ways to model more complicated source features. Generative/data-driven modeling of background galaxies could easily be interfaced with our pipeline to account for the variation in structure of the background source~\citep{1901.01359}.

\begin{figure}
\centering
\includegraphics[height=0.4\textwidth]{figures/posterior}
\caption{Expected posterior and $95\%$ credible regions for 100 observed lenses. The mock observations are generated for $\fsub = 0.05$ and $\beta -0.9$. We assume a uniform prior on $\fsub$ and a Gaussian prior with mean $-0.9$ and standard deviation $0.1$ for $\beta$.}
\label{fig:bayesian_post}
\end{figure}

Similarly, the host lens can be made more realistic by relaxing the restriction to spherical host halos and including more complicated profiles than the Singular Isothermal Sphere used here, drawing and marginalizing over additional host parameters as required. External shear, which models the fact that the local large-scale structure environment of the host galaxy can induce an additional overall deflection field in a preferred direction, can similarly be parameterized and marginalized over~\citep{1997MNRAS.292..673S,astro-ph/9610163}.

A realistic simulator should also model the dynamical evolution of subhalos~\citep{2017MNRAS.469.1997D}, which could entail drawing the positions of the subhalos in three dimensions relative to the host center rather than just in two. Effects associated with tidal disruption due to the large gradient of the galactic potential towards the center of the host galaxy is expected to deplete the fraction of mass bound in substructures there, leading to a depressed overall subhalo abundance~\citep{2016MNRAS.457.1208H} with profile properties (e.\,g., concentration~\citep{1603.04057} and a truncation radius~\citep{0705.0682}) that depend on the distance from the host center. Our subhalo mass function in Eq.~\eqref{eq:shmf} is independent of the lens redshift, but can easily be extended to include this dependency~\citep{2018PhRvD..97l3002H,2017MNRAS.469.1997D}.

All of these effects are straightforward to implement in our setup and only require modifications to the simulation code. The inference algorithm is unaffected; since these extensions do not explicitly depend on the parameters of interest, the likelihood terms associated with them cancel in the calculation of the joint likelihood ratio and the joint score. Nevertheless, these changes affect the final observed image and therefore also the true likelihood function; the variance of the joint likelihood ratio and score could therefore increase, requiring larger training samples before the network converges to the correct likelihood ratio function.

With these extensions, the redshift of the background source and the lens will play a more important role. Since these redshifts are potentially observable through spectroscopic follow-up observations, it is likely that we can improve the performance of the inference algorithm by using this information. We can treat both the source and lens redshift, potentially with added uncertainty to model measurement noise, as additional observables. The input to the neural networks then consists of the observed lens image, the measured (potentially noisy) redshifts, and the tested parameter point. Except for a simple modification of the network architecture, the inference algorithm remains unchanged.

Including line-of-sight substructure can be somewhat more involved, since it necessitates the introduction of a separate line-of-sight halo mass function~\citep{1610.01599,1710.05029,1901.11031,2019arXiv190504182H}. Depending on the specific model (and whether foreground substructure is treated as a nuisance effect or additional signal to be leveraged) its parameters could depend on the parameters of interest, which would require a modification of the calculation of the joint likelihood ratio and joint score. Structurally this is identical to our current modeling of subhalos within the lens. Since the abundance of foreground substructure is expected to be at most comparable to the substructure within the lensing galaxy (depending on the source redshift), we expect that these additional factors in the joint likelihood ratio and joint score will not slow down the overall simulation significantly, and will not increase the variance of the inference techniques too much while having the potential to improve the overall sensitivity of the analysis to substructure abundance in the Universe.

It is expected that a sample of strong lenses will include image-to-image variations on the exposure (through observation time), sky background and detector effects like the points spread function (PSF) depending on the specific scanning strategy of the observatory~\citep{2015ApJ...811...20C}. The sky background can be marginalized over as usual. Rather than treating the exposure and PSF model as nuisance parameters, passing them as additional inputs to the network in addition to normalizing the network input to unit exposure is likely to improve performance. Multiple color bands can easily be modeled and included as inputs to the neural network as different color channels, something that is commonly done when using the ResNet architecture we consider. This can substantially improve discrimination between light from the source, host, and sky background.

While including these extensions in our simulation and inference code is feasible, the detailed modeling is beyond the scope of the current paper. We leave the implementation of these features and application to real lensing data to future work.

% While most of the changes are likely to reduce the information that can be extracted from the lens image, providing the redshifts as additional input, using multiple passbands, and line-of-sight substructure sensitive to the same parameters might in fact provide additional information.


\section{Conclusions}
\label{sec:conclusions}

Strong lensing offers a unique way to probe the properties and distribution of dark matter on sub-galactic scales through the subtle imprint of substructure on lensed arcs. The high dimensionality of the underlying latent space characterizing substructure poses a significant challenge, however. In this paper, we have introduced a novel simulation-based method for inferring high-level population properties characterizing the distribution of substructure in an ensemble of galaxy-galaxy strong lenses based on the ideas introduced in~\citet{1805.00013,1805.00020,1805.12244,Stoye:2018ovl}.

Our results on simulated data demonstrate that such machine learning-based calibrated likelihood ratio estimators offer a promising way to analyze extended-arc strong lensing images with the goal of inferring properties of dark matter substructure. Our proposed method offers several combined advantages over established techniques. In probing the collective effect of a large number of low-mass, sub-threshold subhalos it can offer sensitivity to the faint end of the subhalo mass function where deviations from the concordance CDM paradigm are most likely to be expressed. It can naturally be applied to perform fast, principled, and concurrent analyses of a large sample of strong lenses that share a common set of hyperparameters describing the underlying substructure population properties. Furthermore, rigorous selection of lensing images out of a large sample is not necessary within our framework since images with a smaller effective arc area or low overall fidelity simply do not contribute significantly to the combined substructure analysis, and non-detections are just as valuable as detections. Finally, our analysis is performed at the level of image data, without incurring loss of information associated with dimensionality reduction.

Although we have focused on a simple proof-of-principle example in this paper, extensions to more realistic scenarios --- including more complicated descriptions of the host, source, and substructure populations --- are easily admitted within our framework. The flexibility of the proposed method allows for applications beyond substructure population inference. For example, a large lens sample can be used to perform cosmological parameter estimation while accounting for substructure effects and in particular to independently constrain the Hubble constant~\citep{1907.04869,1907.02533} through its dependence on the angular diameter distance scales in lensing systems.

The code used to obtain the results in this study is available at \url{https://github.com/smsharma/StrongLensing-Inference} \href{https://github.com/smsharma/StrongLensing-Inference}{\faGithub}.


\acknowledgements

We thank Simon Birrer, Christopher Fassnacht, Daniel Gilman, Siavash Golkar, and Neal Weiner for useful conversations. JB and KC are partially supported by NSF awards ACI-1450310, OAC-1836650, and OAC-1841471, and the Moore-Sloan data science environment at NYU. SM is partially supported by the NSF CAREER grant PHY-1554858 and NSF grant PHY-1620727. KC is also supported through the NSF grant PHY-1505463. This work was also supported through the NYU IT High Performance Computing resources, services, and staff expertise. This research has made use of NASA's Astrophysics Data System.

\software{
\package{Astropy} \citep{2013A&A...558A..33A,2018AJ....156..123A},
\package{Jupyter} \citep{Kluyver2016JupyterN},
\package{IPython} \citep{PER-GRA:2007},
\package{LensPop} \citep{2015ApJ...811...20C},
\package{MadMiner} \citep{Brehmer:2019xox},
\package{matplotlib} \citep{Hunter:2007},
\package{NumPy} \citep{numpy:2011},
\package{PyTorch} \citep{paszke2017automatic},
\package{SciPy} \citep{Jones:2001ab}.
}


\appendix
\section{Minimum of the loss functional}
\label{app:variation}

A central step in our inference technique is numerically minimizing the functional $L[g(x, \stattheta)]$ given in Eq.~\eqref{eq:alices_loss} to obtain an estimator for the likelihood ratio function. Here we will use calculus of variation to explicitly show that the solution given in Eq.~\eqref{eq:loss_minimum} in fact minimizes this loss.

First consider the case of $\alpha = 0$, \ie the functional
%
\begin{align}
  L[g(x, \stattheta)]
  &= \int\!\!\diff\stattheta\!\!\int\!\!\diff\stattheta'\!\!\int\!\!\diff x\!\! \int\!\!\diff z \, \pi(\stattheta) \pi(\stattheta') p(x,z|\stattheta)
  \Bigl(- s \log g  - (1 - s) \log (1 - g) - s' \log g'  - (1 - s') \log (1 - g') \Bigr) \notag \\
  &= \int\!\!\diff\stattheta\!\!\int\!\!\diff x
  \underbrace{ \Biggl[
    \int\!\!\diff z \, \pi(\stattheta) \, \Bigl( p(x,z|\stattheta) + p_{\mathrm{ref}}(x,z) \Bigr)
    \Bigl(- s \log g  - (1 - s) \log (1 - g) \Bigr)
  \Biggr] }_{
  \equiv F(x,\stattheta)
  } \,,
\end{align}
%
where we use the shorthand notation $s \equiv s(x,z|\stattheta) \equiv 1 / (1 + r(x,z|\stattheta))$,  $s' \equiv s(x,z|\stattheta') \equiv 1 / (1 + r(x,z|\stattheta'))$, $g \equiv g(x, \stattheta)$, $g' \equiv g(x, \stattheta')$. The function $g^*(x|\stattheta)$ that minimizes  this functional has to satisfy
%
\begin{equation}
  0 \stackrel{!}{=} \frac {\delta F}{\delta g} \Biggr|_{g^*}
  =  \int\!\!\diff z \, \pi(\stattheta) \Bigl( p(x,z|\stattheta) + p_{\mathrm{ref}}(x,z) \Bigr) \Bigl( - \frac s {g^*} + \frac {1-s}{1-g^*} \Bigr)
\end{equation}
%
As long as $\pi(\stattheta) > 0$, this is equivalent to
%
\begin{equation}
  (1-g^*) \int\!\diff z \Bigl( p(x,z|\stattheta) + p_{\mathrm{ref}}(x,z) \Bigr) s
  = g^* \int\!\diff z \Bigl( p(x,z|\stattheta) + p_{\mathrm{ref}}(x,z) \Bigr) (1-s)
\end{equation}
%
and finally
%
\begin{align}
  g^*(x | \stattheta)
  &= \frac {\int\!\diff z \, \Bigl(p(x,z|\stattheta)+\pref(x,z)\Bigr) s(x,z|\stattheta)} {\int \! \diff z \, \Bigl(p(x,z|\stattheta)+\pref(x,z)\Bigr)} \notag \\
  &= \frac
  {\int\!\diff z \, \Bigl(p(x,z|\stattheta) + \pref(x,z)\Bigr) \frac{1}{1+p(x,z|\stattheta)/p_{\mathrm{ref}}(x,z)}}
  {\int\!\diff z \, \Bigl(p(x,z|\stattheta) + \pref(x,z)\Bigr)} \notag \\
  &= \frac {\pref(x)} {p(x|\stattheta) + \pref(x)}
  = \frac{1}
  {1 + r(x|\stattheta)} \,,
\end{align}
%
in agreement with Eq.~\eqref{eq:loss_minimum}. Note that this result is independent of the choise of $\pi(\stattheta)$, as long as this proposal distribution has support at all relevant parameter points.

Similarly it can be shown that the gradient term in the loss functional weighted by $\alpha$ is minimized when the gradient of the log likelihood ratio estimated by the neural network is equal to the true score,
%
\begin{equation}
  \nabla_\stattheta \log \hat{r}(x|\stattheta) \equiv \nabla_\stattheta \log \frac {1 - g^*(x, \stattheta)}{g^*(x, \stattheta)} = \nabla_\stattheta \log r(x|\stattheta) \,.
\end{equation}
%
We refer the reader to \citep{1805.00020} for the derivation. While not strictly necessary for the inference technique, including this term in the loss function substantially improves the sample efficiency of the algorithm, similar to how gradient information makes any fit converge faster.


\section{Simplified scenarios}

\begin{figure*}
\centering
\includegraphics[height=0.4\textwidth]{figures/scenarios_2d}
\includegraphics[height=0.4\textwidth]{figures/scenarios_1d}
\caption{Left: Expected $95\%$~CL exclusion limits for 5 observed lenses for four different levels of complexity of the simulator. Right: expected likelihood ratio along a one-dimensional slice through the parameter space at $\beta = -0.9$ for the same four simulator scenarios. In both panels we compare the ``full'' simulator discussed in Sec.~\ref{sec:lensing-formalism}, a scenario in which the host mass is varied but the offset relative to the source is fixed at zero (``mass''), a case in which the source offset is varied but the host halo mass is fixed (``align''), and a toy scenario in which both the offset and the mass of the host halo are fixed (``fix''). The data was generated for $\beta = -0.9$ and $\fsub = 0.05$.}
\label{fig:scenarios}
\end{figure*}

In order to validate our setup and to disentangle the impact of different latent variables on the inference results we consider three additional versions of our simulation. In the simplest one, which we call ``fix'', all source and host properties are fixed to particular value, including the host halo mass and the offset between source and lens, which is set to zero. In the ``align'' scenario we relax the restriction on the source offset variables $\theta_x$ and $\theta_y$ and draw them from a Gaussian as described in Sec.~\ref{sec:lensing-formalism}. In the ``mass'' version, on the other hand, the offset is fixed at zero, but the host halo mass is drawn from a distribution as described above. We train separate neural networks on lens images generated in these three scenarios and calculated likelihood maps as described in Sec.~\ref{sec:lfi-formalism}, although to save computation time we do not perform a calibration procedure.

The expected confidence limits for 5 observed lens images in the three simplified scenarios and our ``full'' setup are compared in Fig.~\ref{fig:scenarios}. As expected, the more latent variables we keep fixed, the more the inference technique becomes more sensitive to the parameters of interest. In particular fixing the source-host alignment substantially increases the strength of the expected limits.


\bibliographystyle{aasjournal_mod}
% \bibliographystyle{lensing-lfi}
\bibliography{lensing-lfi}

\end{document}
